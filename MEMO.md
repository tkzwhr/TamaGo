# 歴史

- 21世紀になってもなかなかアマ級位者の域を出なかった
- ここ数年であっという間にプロを超えてしまった
- Deep Learningによるブレイクスルー

# 技術

- min-max法
  - 今ではほぼ使われてない
    - 正確な局面評価関数の作成が困難
    - 必要な読む手が膨大すぎる
- モンテカルロ木探索
  - 対局シミュレーションを局面評価関数に使用する
  - 途中まで打ったら最後までシミュレーションしてみる
  - 着手評価にはUCB1値を使う
  - 弱点
    - 大差がついていると変な着手をしがち
    - 責め合いやシチョウでは信頼性が落ちる
- Deep Learning
  - Neural Network 画像認識でも使われている
  - 囲碁は画像に近い
  - 推測したいもの
    - Policy どこに打つといいか
    - Value 勝率
    - Ownership どこが陣地っぽいか
    - Score 目数
    - 難しさは S < V < O < P
- AlphaGo
  - モンテカルロ木探索 + Deep Learning
  - PolicyとValueのネットワークを利用
    - Policyは人間の棋譜から教師あり学習
    - Valueは自己対戦と強化学習
- AlphaGo Zero
  - 人間が生み出したデータは使わない
  - PolicyとValueを統合したDual Networkを使う
  - 膨大な計算機資源を使う
  - 自己対戦ではPUCTアルゴリズム（PUCB値）を使う
- Alpha Zero
  - 考え方を他のゲームに拡大
- Gumbel AlphaZero
  - TamaGoでも採用
  - AlphaZeroでは時間と金がかかりすぎる
    - 自己対戦でのデータ生成がHeavy
    - 計算回数を減らせばいいのでは？
      - ノイズの影響が大きくなる
      - Valueが過学習を起こす
  - 自己対戦での探索回数が少なくても学習が進められる
    - ただし、最終的な強さはAlphaZeroよりは弱い
  - SHOT (Sequential Halving applied to trees)
  - Improved Policy

# TamaGo

- 概要
  - モチベーション
    - 改造にスキルが必要
    - 強化学習を手軽にできるように
    - Rayのノウハウの公開
  - 囲碁AIの3要素
    - GTP
    - 碁盤データ
    - 思考処理

# 教師あり学習

- GPUの使用を推奨
- 棚瀬さん提供の約5万の棋譜データがある
- 学習率
  - 最初は大きく、そこから徐々に小さくしていくことがポイント
  - EPOCHSと併せて調整する
- 他のハイパーパラメータ
  - 変更しない方がいい
  - npzファイルがエラーを吐いたらDATA_SET_SIZEを小さくする

# 強化学習

- GPUの使用がほぼ必須
- 終局判定にはGNUGoを使ったほうが良い
- 学習率
  - 手で数値を変える必要がある
  - 己の見極めが大事

# TamaGoを強くするには

- 共通
  - NNのフィルタ数、ブロック数を増やす
  - ニューラルネットワークの入力特徴を増やす
  - ResBlock → SEBlock
  - ReLU → Swish
  - Pythonをやめる
    - 読みが深くなる
    - 学習サイクルが速くなる
- 教師あり学習
  - データを増やす
    - 棋譜ファイルを増やす
    - 局面を回転・反転させる
  - Optimizerを変える
- 強化学習
  - 自己対戦の探索回数
    - 16 → 32でも結構改善する
  - 対戦数を増やす
    - ただし、80万局生成しており、70万局からあまり変化していないので、頭打ちの可能性がある

# 入力特徴に関する質問への回答

### ① 入力特徴の入力順序について

> 現状で与えている6種類の特徴の入力順は学習に影響があるのでしょうか？

入力順は学習に影響しないので大丈夫です。新しい特徴を追加するときに7番目, 8番目, ..., と末尾に挿入するケースでも"自分の石の特徴をまとめたい"と2番目と3番目の間に挿入するケースでも特に変わりなく学習します。

### ② 入力する特徴の値について

> 特徴に0や1、-1以外の数値を与えることはどのような意味を持つのでしょうか？

あまり大きすぎる値を入力すると学習が破綻することがあったり、小さすぎる値を入力するとその特徴が学習されなかったりという現象は見られます。かなり極端な値を与えた時に発生するので、自分の石の値だけを100にするとニューラルネットワークの計算途中でInfやNaNが発生して破綻する可能性があります。
一方で、ダメが1つの連の座標を1, ダメが2つの連の座標を2としてもニューラルネットワークがその入力特徴の値に合わせて学習するので、極端な値になりすぎなければ入力として与えても問題ありません。よく使われるのはある基準値を決めて、それが1になるように値を割り算してあげるテクニックです。
入力特徴の値の設計については値をそのまま入力する方法とOne-Hot encodingという手法の2通りが考えられます。
例えば、5路盤の例の局面の相手の連の呼吸点数について
```
0, 0, 2, 0, 0
0, 3, 0, 0, 0
0, 3, 0, 0, 0
2, 0, 0, 0, 0
0, 0, 0, 0, 0
```
と入力するのは値をそのまま入力するケース。
呼吸点数が1つの相手の連の座標
```
0, 0, 0, 0, 0
0, 0, 0, 0, 0
0, 0, 0, 0, 0
0, 0, 0, 0, 0
0, 0, 0, 0, 0
```
呼吸点数が2つの相手の連の座標
```
0, 0, 1, 0, 0
0, 0, 0, 0, 0
0, 0, 0, 0, 0
1, 0, 0, 0, 0
0, 0, 0, 0, 0
```
呼吸点数が3つ以上の相手の連の座標
```
0, 0, 0, 0, 0
0, 1, 0, 0, 0
0, 1, 0, 0, 0
0, 0, 0, 0, 0
0, 0, 0, 0, 0
```
と各入力に当てはまる場所を"Yes (1)", "No (0)"で表現するのがOne-Hot encodingです。
連の呼吸点数の特徴についてはOne-Hot encodingを使うのが一般的です。きちんと計測したことはありませんが、One-Hot encodigの方が精度が出ると思います。

### ③ 手番の特徴が1, 0ではなく、1, -1になっていることについて

> 手番が白番のときは0ではなく-1にしているのはどんな意図があるのでしょうか？

黒番か否かで入力特徴を作る場合は前述したOne-Hot encodingにする場合は1, 0になりますが、この特徴については厳密には手番ではなくコミの値に着目して作っています。その手番から見たコミの値で考えるとコミ7目は黒番にとってはそのまま7目、白番にとってはコミをもらっている立場なので-7目とみなせます。あまり大きな値になると良くないので、7で割った値を入力特徴としているため、黒番は1、白番は-1になっています。
Rayの強化学習では色々なコミの値で学習させており、7目を基準に7.5目を学習するときには1.0714..., -1.0714の値を取ったりしている設計をTamaGoに流用しているため、このような値の取り方になっています。


# 学習に使えそうな棋譜

- http://www.yss-aya.com/deep_go.html#08
  - 囲碁クエストの2018年当時の棋譜。9路は25万棋譜ほど
- https://katagoarchive.org/
  - KataGoの現在の学習棋譜。19路がメインですが9路も含まれます。「kata1」「traininggames」にSGF形式があります。
- http://www.yss-aya.com/cgos/9x9/archive.html
  - CGOS、囲碁AI同士の棋譜。数は少ない。
- https://drive.google.com/drive/folders/1hNUKX-KFx6P87wuVLMU4cRqPoBu-9KJ9?usp=sharing
  - 夏風(CGOSで2876)の46万棋譜。

# 参考文献

- http://entcog.c.ooco.jp/entcog/contents/lecture/igoAI2023.html
- https://www.youtube.com/watch?v=Vo-kaHGgFLc
- http://entcog.c.ooco.jp/entcog/contents/lecture/20230520_kobayashi.pdf
